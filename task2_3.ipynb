{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: Ayush Kamleshbhai Patel\n",
    "#### Student ID: 3891013\n",
    "\n",
    "Date: XXXX\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "\n",
    "## Introduction\n",
    "You should give a brief information of this assessment task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import FastText\n",
    "from sklearn.decomposition import PCA\n",
    "import gc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import umap.umap_ as umap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('description.txt') as txtf:\n",
    "    article_txts = txtf.read().splitlines() # reading a list of strings, each for a document/article\n",
    "tk_description = [a.split(' ') for a in article_txts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5168"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(chain.from_iterable(tk_description))\n",
    "# set of unique words\n",
    "vocab = sorted(list(set(words)))\n",
    "# total number of the vocabulary\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webindex</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "      <th>Tokenized Description</th>\n",
       "      <th>Tokenized Title</th>\n",
       "      <th>Tokenized Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>category_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68997528</td>\n",
       "      <td>Finance / Accounts Asst Bromley to ****k</td>\n",
       "      <td>First Recruitment Services</td>\n",
       "      <td>Accountant (partqualified) to **** p.a. South ...</td>\n",
       "      <td>['accountant', 'partqualified', 'south', 'east...</td>\n",
       "      <td>['finance', 'accounts', 'asst', 'bromley', 'to...</td>\n",
       "      <td>['first', 'recruitment', 'services']</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68063513</td>\n",
       "      <td>Fund Accountant  Hedge Fund</td>\n",
       "      <td>Austin Andrew Ltd</td>\n",
       "      <td>One of the leading Hedge Funds in London is cu...</td>\n",
       "      <td>['hedge', 'funds', 'london', 'recruiting', 'fu...</td>\n",
       "      <td>['fund', 'accountant', 'hedge', 'fund']</td>\n",
       "      <td>['austin', 'andrew', 'ltd']</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68700336</td>\n",
       "      <td>Deputy Home Manager</td>\n",
       "      <td>Caritas</td>\n",
       "      <td>An exciting opportunity has arisen to join an ...</td>\n",
       "      <td>['exciting', 'arisen', 'establish', 'provider'...</td>\n",
       "      <td>['deputy', 'home', 'manager']</td>\n",
       "      <td>['caritas']</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67996688</td>\n",
       "      <td>Brokers Wanted Imediate Start</td>\n",
       "      <td>OneTwoTrade</td>\n",
       "      <td>OneTwoTrade is expanding their Sales Team and ...</td>\n",
       "      <td>['expanding', 'recruiting', 'junior', 'trainee...</td>\n",
       "      <td>['brokers', 'wanted', 'imediate', 'start']</td>\n",
       "      <td>['onetwotrade']</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71803987</td>\n",
       "      <td>RGN Nurses (Hospitals)  Penarth</td>\n",
       "      <td>Swiis Healthcare</td>\n",
       "      <td>RGN Nurses (Hospitals) Immediate fulltime and ...</td>\n",
       "      <td>['rgn', 'nurses', 'hospitals', 'fulltime', 'pa...</td>\n",
       "      <td>['rgn', 'nurses', 'hospitals', 'penarth']</td>\n",
       "      <td>['swiis', 'healthcare']</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Webindex                                     Title  \\\n",
       "0  68997528  Finance / Accounts Asst Bromley to ****k   \n",
       "1  68063513               Fund Accountant  Hedge Fund   \n",
       "2  68700336                       Deputy Home Manager   \n",
       "3  67996688             Brokers Wanted Imediate Start   \n",
       "4  71803987           RGN Nurses (Hospitals)  Penarth   \n",
       "\n",
       "                      Company  \\\n",
       "0  First Recruitment Services   \n",
       "1           Austin Andrew Ltd   \n",
       "2                     Caritas   \n",
       "3                 OneTwoTrade   \n",
       "4            Swiis Healthcare   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Accountant (partqualified) to **** p.a. South ...   \n",
       "1  One of the leading Hedge Funds in London is cu...   \n",
       "2  An exciting opportunity has arisen to join an ...   \n",
       "3  OneTwoTrade is expanding their Sales Team and ...   \n",
       "4  RGN Nurses (Hospitals) Immediate fulltime and ...   \n",
       "\n",
       "                               Tokenized Description  \\\n",
       "0  ['accountant', 'partqualified', 'south', 'east...   \n",
       "1  ['hedge', 'funds', 'london', 'recruiting', 'fu...   \n",
       "2  ['exciting', 'arisen', 'establish', 'provider'...   \n",
       "3  ['expanding', 'recruiting', 'junior', 'trainee...   \n",
       "4  ['rgn', 'nurses', 'hospitals', 'fulltime', 'pa...   \n",
       "\n",
       "                                     Tokenized Title  \\\n",
       "0  ['finance', 'accounts', 'asst', 'bromley', 'to...   \n",
       "1            ['fund', 'accountant', 'hedge', 'fund']   \n",
       "2                      ['deputy', 'home', 'manager']   \n",
       "3         ['brokers', 'wanted', 'imediate', 'start']   \n",
       "4          ['rgn', 'nurses', 'hospitals', 'penarth']   \n",
       "\n",
       "                      Tokenized Company            Category  category_encoded  \n",
       "0  ['first', 'recruitment', 'services']  Accounting_Finance                 0  \n",
       "1           ['austin', 'andrew', 'ltd']  Accounting_Finance                 0  \n",
       "2                           ['caritas']  Healthcare_Nursing                 2  \n",
       "3                       ['onetwotrade']  Accounting_Finance                 0  \n",
       "4               ['swiis', 'healthcare']  Healthcare_Nursing                 2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_data = pd.read_csv('jobs_data.csv')\n",
    "webindex = jobs_data['Webindex']\n",
    "# Define the mapping\n",
    "category_to_int_mapping = {\n",
    "    'Accounting_Finance': 0,\n",
    "    'Engineering': 1,\n",
    "    'Healthcare_Nursing': 2,\n",
    "    'Sales': 3\n",
    "}\n",
    "\n",
    "# Replace the labels with their integer values\n",
    "jobs_data['category_encoded'] = jobs_data['Category'].map(category_to_int_mapping)\n",
    "jobs_data['category_encoded'] = jobs_data['category_encoded'].astype(int)\n",
    "jobs_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Binary Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5168)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to perform the task...\n",
    "joined_description = [' '.join(review) for review in tk_description]\n",
    "bVectorizer = CountVectorizer(analyzer = \"word\",binary = True,vocabulary = vocab) # initialise the CountVectorizer\n",
    "binary_features = bVectorizer.fit_transform(joined_description)\n",
    "binary_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5168)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cVectorizer = CountVectorizer(analyzer = \"word\",vocabulary = vocab) # initialised the CountVectorizer\n",
    "count_features = cVectorizer.fit_transform(joined_description)\n",
    "count_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aap</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aat</th>\n",
       "      <th>abb</th>\n",
       "      <th>abenefit</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abi</th>\n",
       "      <th>abilities</th>\n",
       "      <th>abreast</th>\n",
       "      <th>abroad</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>yeovil</th>\n",
       "      <th>yn</th>\n",
       "      <th>york</th>\n",
       "      <th>yorkshire</th>\n",
       "      <th>youmust</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aap  aaron  aat  abb  abenefit  aberdeen  abi  abilities  abreast  abroad  \\\n",
       "0    0      0    0    0         0         0    0          0        0       0   \n",
       "1    0      0    0    0         0         0    0          0        0       0   \n",
       "2    0      0    0    0         0         0    0          0        0       0   \n",
       "\n",
       "   ...  years  yeovil  yn  york  yorkshire  youmust  young  younger  yrs  zest  \n",
       "0  ...      0       0   0     0          0        0      0        0    0     0  \n",
       "1  ...      0       0   0     0          0        0      0        0    0     0  \n",
       "2  ...      0       0   0     0          0        0      0        0    0     0  \n",
       "\n",
       "[3 rows x 5168 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_features_df = pd.DataFrame(count_features.toarray(), columns=cVectorizer.get_feature_names_out())\n",
    "# print out samples\n",
    "count_features_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it into the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vector representation saved to count_vectors.txt\n"
     ]
    }
   ],
   "source": [
    "count_features = cVectorizer.fit_transform(joined_description).toarray()\n",
    "\n",
    "def save_count_vector(count_features, webindex, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(len(count_features)):\n",
    "            f.write('#' + str(webindex[i]) + ',')\n",
    "            for j in range(len(count_features[i])):\n",
    "                if count_features[i][j] != 0:\n",
    "                    f.write(str(j) + ':' + str(count_features[i][j]) + ',')\n",
    "            f.write('\\n')\n",
    "    f.close()\n",
    "    print('Count vector representation saved to ' + filename)\n",
    "\n",
    "\n",
    "save_count_vector(count_features, webindex, 'count_vectors.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5168)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tVectorizer = TfidfVectorizer(analyzer = \"word\",vocabulary = vocab) # initialised the TfidfVectorizer\n",
    "tfidf_features = tVectorizer.fit_transform(joined_description) # generate the tfidf vector representation for all articles\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vectorFile(data_features,filename):\n",
    "    num = data_features.shape[0] # the number of document\n",
    "    out_file = open(filename, 'w') # creates a txt file and open to save the vector representation\n",
    "    for a_ind in range(0, num): # loop through each article by index\n",
    "        for f_ind in data_features[a_ind].nonzero()[1]: # for each word index that has non-zero entry in the data_feature\n",
    "            value = data_features[a_ind][0,f_ind] # retrieve the value of the entry from data_features\n",
    "            out_file.write(\"{}:{} \".format(f_ind,value)) # write the entry to the file in the format of word_index:value\n",
    "        out_file.write('\\n') # start a new line after each article\n",
    "    out_file.close() # close the file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features_file = \"jobs_data_vector.txt\" # file name of the tfidf vector\n",
    "\n",
    "write_vectorFile(tfidf_features,tfidf_features_file) # write the tfidf vector to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model based on word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and Saving FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText<vocab=2741, vector_size=50, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# 1. Set the corpus file names/path\n",
    "corpus_file = 'description.txt'\n",
    "\n",
    "# 2. Initialise the Fast Text model\n",
    "bbcFT = FastText(vector_size=50) \n",
    "\n",
    "# 3. build the vocabulary\n",
    "bbcFT.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# 4. train the model\n",
    "bbcFT.train(\n",
    "    corpus_file=corpus_file, epochs=bbcFT.epochs,\n",
    "    total_examples=bbcFT.corpus_count, total_words=bbcFT.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(bbcFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the KeyedVectors from the model as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextKeyedVectors<vector_size=50, 2741 keys>\n"
     ]
    }
   ],
   "source": [
    "bbcFT_wv = bbcFT.wv\n",
    "print(bbcFT_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model\n",
    "we also save our trained FastText model using the standard gensim methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# bbcFT.save(\"models/FastText/bbcFT.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText<vocab=2741, vector_size=50, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "bbcFT = FastText.load(\"models/FastText/bbcFT.model\")\n",
    "print(bbcFT)\n",
    "bbcFT_wv= bbcFT.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we experiment the FastText embeddings. \n",
    "Similar, we:\n",
    "* load the FastText model saved in our prevoius activity;\n",
    "* generate document embeddings based on the load FastText word embeddings;\n",
    "* explore the reprensentiveness of the features through tSNE;\n",
    "* bulid the logistic regression model based on the generated document embeddings for news classfication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docvecs(embeddings, docs):\n",
    "    vecs = np.zeros((len(docs), embeddings.vector_size))\n",
    "    for i, doc in enumerate(docs):\n",
    "        valid_keys = [term for term in doc if term in embeddings.key_to_index]\n",
    "        docvec = np.vstack([embeddings[term] for term in valid_keys])\n",
    "        \"\"\"\n",
    "        Note: using `sum` here, other 'pooling' options are possible too,\n",
    "        e.g. mean, etc.\n",
    "        \"\"\"\n",
    "        docvec = np.sum(docvec, axis=0)\n",
    "        vecs[i,:] = docvec\n",
    "    return vecs\n",
    "\n",
    "\n",
    "def gen_docVecs(wv,tk_txts):\n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "    #stopwords = nltk.corpus.stopwords.words('english') # if we haven't pre-processed the articles, it's a good idea to remove stop words\n",
    "\n",
    "    for i in range(0,len(tk_txts)):\n",
    "        tokens = tk_txts[i]\n",
    "        temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "        for w_ind in range(0, len(tokens)): # looping through each word of a single document and spliting through space\n",
    "            try:\n",
    "                word = tokens[w_ind]\n",
    "                word_vec = wv[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                temp = pd.concat([temp, pd.Series(word_vec)], ignore_index = True)\n",
    "            except:\n",
    "                pass\n",
    "        doc_vector = temp.sum() # take the sum of each column\n",
    "        docs_vectors = pd.concat([docs_vectors, doc_vector], ignore_index = True)\n",
    "    return docs_vectors\n",
    "\n",
    "def plotTSNE(labels,features): # features as a numpy array, each element of the array is the document embedding of an article\n",
    "    categories = sorted(labels.unique())\n",
    "    # Sampling a subset of our dataset because t-SNE is computationally expensive\n",
    "    SAMPLE_SIZE = int(len(features) * 0.3)\n",
    "    np.random.seed(0)\n",
    "    indices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)\n",
    "    projected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices])\n",
    "    colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey']\n",
    "    for i in range(0,len(categories)):\n",
    "        points = projected_features[(labels[indices] == categories[i])]\n",
    "        plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[i], label=categories[i])\n",
    "    plt.title(\"Feature vector for each article, projected on 2 dimensions.\",\n",
    "              fontdict=dict(fontsize=15))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE this can take some time to finish running\n",
    "# generate document embeddings\n",
    "bbcFT_dvs = gen_docVecs(bbcFT_wv,jobs_data['Tokenized Description'])\n",
    "bbcFT_dvs.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bbcFT_dvs \u001b[39m=\u001b[39m docvecs(bbcFT_wv, jobs_data[\u001b[39m'\u001b[39;49m\u001b[39mTokenized Description\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32m/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb Cell 32\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(docs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     valid_keys \u001b[39m=\u001b[39m [term \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m term \u001b[39min\u001b[39;00m embeddings\u001b[39m.\u001b[39mkey_to_index]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     docvec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvstack([embeddings[term] \u001b[39mfor\u001b[39;49;00m term \u001b[39min\u001b[39;49;00m valid_keys])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m    Note: using `sum` here, other 'pooling' options are possible too,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m    e.g. mean, etc.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y122sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     docvec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(docvec, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/numpy/core/shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    295\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, casting\u001b[39m=\u001b[39;49mcasting)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "bbcFT_dvs = docvecs(bbcFT_wv, jobs_data['Tokenized Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=2 must be between 1 and min(n_samples, n_features)=1 with svd_solver='randomized'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m features \u001b[39m=\u001b[39m bbcFT_dvs\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y131sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plotTSNE(jobs_data[\u001b[39m'\u001b[39;49m\u001b[39mCategory\u001b[39;49m\u001b[39m'\u001b[39;49m],features)\n",
      "\u001b[1;32m/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb Cell 33\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y131sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y131sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(features)), size\u001b[39m=\u001b[39mSAMPLE_SIZE, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y131sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m projected_features \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mfit_transform(features[indices])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y131sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m colors \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mpink\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmidnightblue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morange\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdarkgrey\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ayushpatel/Desktop/RMIT/AdvProgramming/Assignments/a2-milestone1/task2_3.ipynb#Y131sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(categories)):\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \n\u001b[1;32m   1092\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1111\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m   1112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ \u001b[39m=\u001b[39m embedding\n\u001b[1;32m   1113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:984\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39m# Always output a numpy array, no matter what is configured globally\u001b[39;00m\n\u001b[1;32m    983\u001b[0m pca\u001b[39m.\u001b[39mset_output(transform\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 984\u001b[0m X_embedded \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(X)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    985\u001b[0m \u001b[39m# PCA is rescaled so that PC1 has standard deviation 1e-4 which is\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[39m# the default value for random initialization. See issue #18018.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m X_embedded \u001b[39m=\u001b[39m X_embedded \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mstd(X_embedded[:, \u001b[39m0\u001b[39m]) \u001b[39m*\u001b[39m \u001b[39m1e-4\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m    461\u001b[0m     U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[1;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[1;32m    464\u001b[0m         \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:512\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_full(X, n_components)\n\u001b[1;32m    511\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_truncated(X, n_components, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_svd_solver)\n",
      "File \u001b[0;32m~/anaconda3/envs/AdvProgramming/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:585\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    581\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m cannot be a string with svd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[39m%\u001b[39m (n_components, svd_solver)\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_components \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m--> 585\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    586\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m must be between 1 and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmin(n_samples, n_features)=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msvd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m         \u001b[39m%\u001b[39m (n_components, \u001b[39mmin\u001b[39m(n_samples, n_features), svd_solver)\n\u001b[1;32m    590\u001b[0m     )\n\u001b[1;32m    591\u001b[0m \u001b[39melif\u001b[39;00m svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m n_components \u001b[39m==\u001b[39m \u001b[39mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m    592\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    593\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m must be strictly less than \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    594\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmin(n_samples, n_features)=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msvd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m         \u001b[39m%\u001b[39m (n_components, \u001b[39mmin\u001b[39m(n_samples, n_features), svd_solver)\n\u001b[1;32m    597\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=2 must be between 1 and min(n_samples, n_features)=1 with svd_solver='randomized'"
     ]
    }
   ],
   "source": [
    "features = bbcFT_dvs.to_numpy()\n",
    "plotTSNE(jobs_data['Category'],features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the weighted (i.e., TF-IDF weighted) and unweighted vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unweighted Document Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representation as per spectification.\n",
    "- count_vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on buidling classification models based on different document feature represetations. \n",
    "Detailed comparsions and evaluations on different models to answer each question as per specification. \n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to perform the task...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Give a short summary and anything you would like to talk about the assessment tasks here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couple of notes for all code blocks in this notebook\n",
    "- please provide proper comment on your code\n",
    "- Please re-start and run all cells to make sure codes are runable and include your output in the submission.   \n",
    "<span style=\"color: red\"> This markdown block can be removed once the task is completed. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
